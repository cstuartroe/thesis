\chapter{Results and Discussion}

In order to demonstrate results, throughout this section I use a sampling of procedurally generated scatter plots demonstrating correlations between performance metrics and other measurements. All scatter plots, including many not shown in this section, can be accessed at

\texttt{https://github.com/cstuartroe/thesis/tree/master/images/generated}.

I reference combinations of architectures and performance metrics. There are two of each - hard and soft attention mechanism architectures, and accuracy and Levenshtein distance model performance metrics - and so there are four combinations.

I frequently distinguish between results for closely related language pairs and distantly related and unrelated language pairs. Among the 242 language pairs, 22 were closely related, 26 were distantly related, and the other 194 were unrelated.

\section{Overall model performance and architectural comparison}
\label{modelperf}

I conducted learning trials with two architectures from the SIGMORPHON 2019 baseline - hard monotonic and soft attention. Mean and standard deviation for baseline (non-transfer) model accuracy and Levenshtein distance from correct solution across all 21 languages, and the same information for transfer learning models across all 242 language pairs, is given below:

\begin{center}
\begin{tabular}{|c|c||c|c|}
\hline
 & & $\mu$ & $\sigma$ \\
\hline \hline
Baseline Accuracy & hard & 30.6\% & 23.3\% \\
\hline
 & soft & 25.8\% & 17.2\% \\
\hline
Baseline Levenshtein & hard & 2.66 & 1.41 \\
\hline
 & soft & 2.8 & 1.01 \\
\hline
Transfer Accuracy & hard & 48.7\% & 27.9\%\\
\hline
 & soft & 34.9\% & 21.8\%\\
\hline
Transfer Levenshtein & hard & 1.95 & 1.56\\
\hline
 & soft & 2.7 & 1.73\\
\hline
\end{tabular}
\end{center}

Overall, the hard attention model performed better on average across all metrics. However, variability from language to language was quite substantial and on some languages, the soft attention model performed more strongly. On the whole, performance of the hard and soft attention models on a given language or pair were correlated, but not tightly so.

\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/significant/Transfer_Accuracy_(hard)_vs_Transfer_Accuracy_(soft)_all_language_pairs.png}
\centering
\caption{Relationship between the transfer accuracy of hard and soft models, across all 242 language pairs. With r=.77, there is clear correlation of moderately high strength. Comparisons of baseline performance and Levenshtein distances between the two models yielded similar relationships.}
\end{figure}

Comparison of transfer learning with baseline outcomes demonstrates that transfer learning conferred substantial benefits on models: an average of 18.1 percentage points improved accuracy for the hard attention models, and 9.1 percentage points for the soft attention model.

As will be seen in the following results sections, a number of statistical effects only appear for the hard attention models, not for soft attention models. The best explanation I have for this fact is that, given the overall weaker performance boosts that transfer learning seemed to confer to soft attention models, statistical effects may simply have been more difficult to detect for soft attention models, as random performance differences between target languages would be comparatively larger in relation to transfer learning effects.

\section{Relationship between source language model performance and transfer learning efficacy}

To ascertain whether there is any relationship between the accuracy of a pretrained model and the eventual accuracy of fine-tuned models based on it, correlations were sought between the baseline performance for each source language and the average performance of transfer learned models using it as a source language. The tables and graphics given here use accuracy as the metric for assessing both pretrained and trained models, but results based on Levenshtein distance were essentially the same.

There were no statistically significant differences in average performance improvements between source languages, nor any statistically significant correlations between pretrained model accuracy and average fine-tuned model accuracy. This suggests a lack of evidence that any one language is a universally good source language, or that a pretrained model must be highly accurate on the source language to be useful as a basis for transfer learning.

\begin{tabular}{|c|c|c|c|}
\hline
Source Language & SL Baseline Accuracy & \multicolumn{2}{|c|}{TL Accuracy Improvement (hard)} \\
& (hard) & $\mu$ & $\sigma$ \\
\hline
\hline
Basque & 0.037 & -0.148 & 0.246\\
\hline
Georgian & 0.435 & 0.005 & 0.114\\
\hline
Slovak & 0.44 & 0.045 & 0.132\\
\hline
Uzbek & 0.04 & 0.075 & 0.141\\
\hline
Hebrew & 0.23 & 0.077 & 0.096\\
\hline
Danish & 0.598 & 0.111 & 0.105\\
\hline
Navajo & 0.12 & 0.168 & 0.169\\
\hline
Portuguese & 0.437 & 0.177 & 0.147\\
\hline
Turkish & 0.145 & 0.187 & 0.297\\
\hline
Bashkir & 0.745 & 0.198 & 0.243\\
\hline
Zulu & 0.041 & 0.199 & 0.203\\
\hline
Finnish & 0.096 & 0.207 & 0.132\\
\hline
Quechua & 0.248 & 0.259 & 0.193\\
\hline
Crimean Tatar & 0.93 & 0.264 & 0.262\\
\hline
Romanian & 0.239 & 0.275 & 0.099\\
\hline
Swedish & 0.489 & 0.292 & 0.232\\
\hline
Italian & 0.275 & 0.314 & 0.18\\
\hline
Estonian & 0.16 & 0.338 & 0.198\\
\hline
Spanish & 0.359 & 0.349 & 0.128\\
\hline
Czech & 0.214 & 0.37 & 0.22\\
\hline
Arabic & 0.149 & 0.395 & 0.213\\
\hline
\end{tabular}

\begin{tabular}{|c|c|c|c|}
\hline
Source Language & SL Baseline Accuracy & \multicolumn{2}{|c|}{TL Accuracy Improvement (soft)} \\
& (soft) & $\mu$ & $\sigma$ \\
\hline
\hline
Bashkir & 0.348 & -0.083 & 0.191\\
\hline
Slovak & 0.233 & -0.059 & 0.196\\
\hline
Georgian & 0.184 & -0.049 & 0.134\\
\hline
Danish & 0.454 & -0.013 & 0.161\\
\hline
Hebrew & 0.203 & -0.007 & 0.123\\
\hline
Uzbek & 0.41 & 0.011 & 0.123\\
\hline
Basque & 0.062 & 0.028 & 0.178\\
\hline
Crimean Tatar & 0.77 & 0.029 & 0.075\\
\hline
Zulu & 0.043 & 0.034 & 0.19\\
\hline
Turkish & 0.176 & 0.069 & 0.187\\
\hline
Navajo & 0.103 & 0.096 & 0.162\\
\hline
Swedish & 0.36 & 0.119 & 0.143\\
\hline
Portuguese & 0.457 & 0.126 & 0.243\\
\hline
Romanian & 0.185 & 0.129 & 0.172\\
\hline
Finnish & 0.046 & 0.139 & 0.183\\
\hline
Czech & 0.164 & 0.173 & 0.134\\
\hline
Italian & 0.253 & 0.176 & 0.181\\
\hline
Arabic & 0.105 & 0.201 & 0.15\\
\hline
Quechua & 0.423 & 0.225 & 0.175\\
\hline
Spanish & 0.201 & 0.243 & 0.142\\
\hline
Estonian & 0.233 & 0.27 & 0.157\\
\hline
\end{tabular}

\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/insignificant/Average_Target_Language_Accuracy_Improvement_(hard)_vs_Source_Language_Baseline_Accuracy_(hard)_Source_Languages.png}
\centering
\caption{}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/insignificant/Average_Target_Language_Accuracy_Improvement_(soft)_vs_Source_Language_Baseline_Accuracy_(soft)_Source_Languages.png}
\centering
\caption{}
\end{figure}

\newpage

\section{Part of speech distribution similarity}

Using both accuracy rate and Levenshtein distance as performance metrics, greater part of speech distribution similarity seems to predict a larger performance boost from transfer learning in hard attention models, among all language pairs and distantly related and unrelated language pairs. For soft attention models and among closely related languages, there is no clear suggestion in the data of any contradictory effect of part of speech distribution similarity. It seems most likely that (as discussed in \ref{modelperf}) transfer learning effects were similar but too small to statistically detect for soft attention models, and that the sample size of closely related languages was too small to rise to the level of statistical significance. 

\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/significant/Accuracy_Improvement_(hard)_vs_POS_distribution_similarity_all_language_pairs.png}
\centering
\caption{}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/significant/Levenshtein_Improvement_(hard)_vs_POS_distribution_similarity_distantly_related_and_unrelated_language_pairs.png}
\centering
\caption{}
\end{figure}

\section{Relationships between language similarity metrics and model performance}

By and large, the takeaway is that all measurements of language similarity predict more effective transfer learning, \textit{except} similarity of adjectival categories and similarity of inflection shape, although what effects do appear rise to the level of statistical significance less often with soft attention models.

\subsection{Part of speech category overlap}

In general, part of speech category overlap was often predictive of effective transfer learning, though there were more nuanced patterns apparent. 

Using both accuracy rate and average Levenshtein distance as metrics of performance, nominal category overlap was predictive of performance at a statistically significant level only for hard attention models and not among closely related language pairs.

\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/significant/Accuracy_Improvement_(hard)_vs_N_category_overlap_all_language_pairs.png}
\centering
\caption{Nominal category overlap has a positive correlation with hard attention model performance.}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/significant/Levenshtein_Improvement_(hard)_vs_N_category_overlap_distantly_related_and_unrelated_language_pairs.png}
\centering
\caption{Nominal category overlap has a positive correlation with hard attention model performance.}
\label{fig:LIhNd}
\end{figure}


\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/insignificant/Accuracy_Improvement_(soft)_vs_N_category_overlap_all_language_pairs.png}
\centering
\caption{Nominal category overlap has no substantial correlation with soft attention model performance.}
\end{figure}

In fact, for soft attention models, the data was not at all suggestive of a relationship with model performance.

Even given the conditions on the relationship between nominal category overlap and transfer learning efficacy, it appears impossible to dismiss what statistically significant relationships do appear. Figure \ref{fig:LIhNd} demonstrates that, among all 220 distantly related and unrelated pairs, nominal category overlap is predictive of transfer learning efficacy, and there are no obvious clustering or outlier effects at work. 

Verbal category overlap was broadly predictive of effective transfer learning across both architectures and metrics of performance, but unlike with nominal category overlap, the effect appeared to be driven entirely by differences among closely related language pairs, and differences between closely related language pairs and other language pairs as groups. The data was not at all suggestive of a relationship between verbal category overlap and model performance when limited to distantly related and unrelated pairs.

\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/significant/Accuracy_Improvement_(soft)_vs_V_category_overlap_all_language_pairs.png}
\centering
\caption{Verbal category overlap is predictive of model performance among all language pairs, but comparison with the next two figures makes it clear that the relationship is characterized by the difference of the rightmost cluster, entirely composed of closely related language pairs, from the rest of the pairs.}
\label{fig:AIsVa}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/significant/Accuracy_Improvement_(soft)_vs_V_category_overlap_closely_related_language_pairs.png}
\centering
\caption{The main cluster of language pairs visible here can be seen as well in Figure \ref{fig:AIsVa}.}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/insignificant/Accuracy_Improvement_(soft)_vs_V_category_overlap_distantly_related_and_unrelated_language_pairs.png}
\centering
\caption{With the cluster of closely related languages removed, verbal category overlap is no longer predictive of performance.}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/significant/Levenshtein_Improvement_(hard)_vs_V_category_overlap_closely_related_language_pairs.png}
\centering
\caption{Verbal category overlap is predictive of effective transfer learning for closely related language pairs, regardless of model architecture and performance metric.}
\end{figure}

It appears from the scatter plots of verbal category overlap and model improvements among closely related languages that the positive correlation is driven primarily by six outlying pairs with substantially lower verbal category overlap than the others; these pairs are Bashkir $\leftrightarrow$ Crimean Tatar, Czech $\leftrightarrow$ Slovak, and Arabic $\leftrightarrow$ Hebrew. Given that this relationship boils down to three pairs of closely related language which happen to have substantially different verbal systems (at least as represented by the input data), it is probably best to avoid drawing too strong a conclusion.

The only robust correlation between verbal category overlap and model performance, then, is the separate clustering of closely related languages and others, and seems likely to simply be reflective of the facts that closely related languages are much more likely to have similar verbal systems and that genealogical similarity is predictive of effective transfer learning. That is, despite many statistically significant relationships between verbal category overlap and transfer learning efficacy, this appears on close examination to be a case of mutual correlation with genealogical similarity.

A similar picture appears for adjectival category overlap, though even more distinctly so. Adjectival category overlap predicts effective transfer learning as measured by average Levenshtein distance among closely related languages, but this seems to be entirely driven by the outlying pairs Bashkir $\leftrightarrow$ Crimean Tatar and Czech $\leftrightarrow$ Slovak. No broader conclusions should be drawn about the relationship between adjectival category overlap and transfer learning.

\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/significant/Levenshtein_Improvement_(soft)_vs_ADJ_category_overlap_closely_related_language_pairs.png}
\centering
\caption{The four rightmost data points are Bashkir $\leftrightarrow$ Crimean Tatar and Czech $\leftrightarrow$ Slovak. The adjectival category overlap of 0 among most pairs is usually due to one or both languages not having any adjectives in their data set; adjectives were less common by far than nouns and verbs across all the input data sets.}
\end{figure}

\subsection{Inflection shape}

\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/insignificant/Accuracy_Improvement_(hard)_vs_Inflection_shape_similarity_all_language_pairs.png}
\centering
\caption{}
\end{figure}

Across all metrics of performance and language pair subsets tested, any correlation between inflection shape similarity and transfer learning efficacy never rose to a statistically significant level. Of all metrics of language similarity assessed in this study, inflection shape is certainly the most unambiguously lacking in correlation with effective transfer learning. From this, it seems straightforward to conclude that there is no evidence of similarity in inflection shapes exhibited by two languages being predictive of effectiveness as a transfer learning pair, at least with the transfer learning strategies tested here.

\subsection{Fusion}

Viewed across the dataset of all language pairs tested, similarity in morphological fusion had a statistically significant positive correlation with transfer learning improvement for all combinations of architecture and performance metric. It was also significant across the dataset of only closely-related language pairs in three out of four architecture $\times$ performance metric combinations.

\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/significant/Levenshtein_Improvement_(hard)_vs_Fusion_similarity_all_language_pairs.png}
\centering
\caption{This was the combination of architecture and performance metric for which the correlation with performance improvement, among all language pairs, was weakest. For all three other combinations, correlation was significant at a $p<.01$ level.}
\end{figure}

However, for the data set of distantly related and unrelated languages, the correlation of fusion similarity and transfer learning efficacy was only significant for one combination: soft attention architecture and accuracy, with $p=.004$, while for all other combinations the $p$-value was greater than .15. Unlike with part of speech category overlaps, this is not apparently due to separate clustering of closely related languages. The data sets with fusion similarity which did not rise to any level of statistical significance all still show a similar positive correlation, and so it may be that the effect exists and simply went undetected for those data sets. 

It is still unclear why the effect was most pronounced for the combination of soft attention and accuracy as a performance metric.

\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/insignificant/Levenshtein_Improvement_(hard)_vs_Fusion_similarity_closely_related_language_pairs.png}
\centering
\caption{Although Levenshtein improvement $\times$ hard attention $\times$ fusion similarity shows a statistically significant correlation among all languages, it does not among the subset of closely related languages. Given that there is still a similar trendline, it may be that the sample size here is simply too small to detect the effect.}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/insignificant/Levenshtein_Improvement_(hard)_vs_Fusion_similarity_distantly_related_and_unrelated_language_pairs.png}
\centering
\caption{Although Levenshtein improvement $\times$ hard attention $\times$ fusion similarity shows a statistically significant correlation among all languages, it does not among the subset of distantly related and unrelated languages.}
\end{figure}

\subsection{Genealogical distance}

For all combinations of architecture and performance metric, genealogical similarity was correlated with more effective transfer learning pairs. That is, a source language more closely related to a particular target language was most likely to result in a higher-performing transfer learned model by all metrics.

\begin{figure}[ht]
\includegraphics[width=8cm]{images/generated/insignificant/Levenshtein_Improvement_(hard)_vs_Genealogical_similarity_all_language_pairs.png}
\centering
\caption{This was the combination of architecture and performance metric for which the correlation between genealogical similarity performance improvement was weakest. For all three other combinations, correlation was significant at a $p<.005$ level.}
\end{figure}

Separate data sets for closely related languages and distantly related and unrelated languages were not considered for this explanatory variable, for obvious reasons.