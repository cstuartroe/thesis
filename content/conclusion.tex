\chapter{Conclusion and Future Work}

The goal of this study was to disentangle some of the factors influencing whether a particular high-resources language is a good candidate for a source language to conduct transfer learning with a particular low-resource target language. This research question has value to computational linguists attempting to develop new morphologically savvy models of low-resource languages. As suggested by \cite{McCarthy2019}, this study produced evidence that a source language closely related to the target language is moderately predictive of effective transfer learning. It also elucidated other types of similarities relevant to transfer learning. 

Broadly, it appears that the largest hurdles for the types of LSTM models tested here center around interpreting grammatical categories, rather than understanding spelling and word structure of languages. Anecdotally, when viewing many of the incorrect predictions that my test models produced, they often corresponded to other correct forms in the target language but didn't match the intended set of grammatical categories; at the least, they typically appeared to be consistent with the phonology and spelling rules of the language, and fairly close to the lemma and/or the correct form. I would interpret many of the statistical results of this study as being consistent with that conjecture: similarity in patterns of grammatical fusion was moderately predictive of transfer learning efficacy, and the most predictive explanatory variable was similarity between source and target language in the overall set of grammatical categories exhibited on nouns. In contrast, similarity in inflection shape has no perceivable bearing on model performance. Lastly, the statistical relationship between part of speech distribution similarity and transfer learning efficacy suggests that grammatical tag similarities between the \textit{data sets} for source and target language, separate from their actual structure, bears on the usefulness of transfer learning. On the basis of these facts, my distillation of the suggestions of the results of this study are:

\begin{center}
    When selecting a source language for transfer learning of a particular target language, morphophonological and inflection shape similarities appear to be less relevant than similarities in the grammatical structure of the two languages and similarities in the set of grammatical tags in the source and target data sets.
\end{center}

That said, there are questions raised by this study that I consider to be not fully answered, in particular questions of the relationship between attention mechanism and linguistic characteristics in determining model performance. Overall, hard attention models better leveraged transfer learning and showed stronger performance overall, and showed a relatively stronger sensitivity to nominal category overlap, while soft attention models showed a greater sensitivity to fusion pattern similarities. These questions are indirectly addressed by \cite{Cotterell2018b} and \cite{McCarthy2019} as they compare the performance of models which vary in design, but those studies do not isolate attention as a variable or explicitly take linguistic properties into account.

I believe that research on that front could be pursued by various further analyses of the data generated here, such as comparing differences in performance between hard and soft attention models with actual linguistic properties of languages, rather than simply similarity of those properties as done here. Study could also be done using new model architectures that explicitly takes into account linguistic typological information such as that used in this study.

On that note, a ripe area for further research involves making model design choices not considered here. For instance, while \cite{McCarthy2019} used a low-resource setting of 100 training examples for target languages, this study used a resource setting of 1000 training examples for each target language, in large part because typical accuracy rates at the lower data setting were too low to reliably detect statistical trends. This may be related to intrinsic learning curve weaknesses of LSTMs (\cite{Cotterell2017a}, \cite{Cotterell2018b}), and so a study similar to this one that makes use of ensembling to ameliorate this issue may be of value, since models of very low-resource languages are likely to make use of some type of ensembling.

Another design choice that has not been previously investigated for this family of computational morphology problems is the use of multiple source languages. It seems perfectly probable that pretraining with several source languages, perhaps languages similar to the target language in different ways, could further push the potential of models. 