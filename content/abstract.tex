% !TEX root = ../thesis-example.tex
%
\pdfbookmark[0]{Abstract}{Abstract}
\chapter*{Abstract}
\label{sec:abstract}
\vspace*{-10mm}

In the SIGMORPHON 2019 shared task 1, multiple teams attempted for the first time to leverage transfer learning to build more accurate models of natural language morphology with small amounts of target language data, with the intended goal of boosting modeling resources for low-resource languages. It was found that transfer learning could aid the development of computational models for low resource languages and that transfer learning was most effective between genealogically related languages. This study expounds on those findings by testing a much larger number of unrelated language pairs, systematically comparing two model architectures, and examining relationships between model performance and selected linguistic typological similarities of source and target languages. It was found that transfer learning can still afford substantial benefits when source and target language are unrelated, and that transfer learning is most beneficial when source and target language have similar sets of morphologically inflected categories and  similar patterns of fusion between those categories, while similarities in inflection shape are not predictive of transfer learning efficacy. This information can be used to select source languages when leveraging transfer learning to improve computational resources for low-resource target languages, especially those without closely related high-resource languages.